{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd56713",
   "metadata": {},
   "source": [
    "# Report on BG CV test task\n",
    "\n",
    "## Artyom V. Vasiliev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a377a42",
   "metadata": {},
   "source": [
    "#### this notebook has the following structure:\n",
    "\n",
    "* [0.Problem description](#description)\n",
    "* [1.0. CNN Image classifier](#image-classifier)\n",
    "* [1.1. - Data overview](#image-classifier-data-overview)\n",
    "* [1.2. - Data preprocessing](#image-classifier-data-preprocessing)\n",
    "* [1.3. - Transfer Learning approach](#image-classifier-transfer-learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c9735",
   "metadata": {},
   "source": [
    "## 0. Problem description <a class=\"anchor\" id=\"description\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58c4ee",
   "metadata": {},
   "source": [
    "**Problem 1:**\n",
    "> Используя фреймворк pytorch, реализуйте архитектуру для\n",
    "классификации изображений по стилям художников. Для решения задачи можно\n",
    "взять достаточно простые архитектуры.\n",
    "Изображения для обучения необходимо уменьшить до размера 64x64 или\n",
    "128x128 (при наличии ресурсов). Обучить реализованную модель на\n",
    "предобработанном датасете. Нарисовать графики изменения значений лосса и\n",
    "выбранных метрик по эпохам.\n",
    "Результаты представить в Jupyter ноутбуке.\n",
    "\n",
    "**Problem 1 extras:** \n",
    "> Вместо выхода сети-классификатора используйте одномерный вектор с\n",
    "предпоследнего слоя сети. Используя этот вектор, как набор признаков для каждого\n",
    "изображения, попробуйте кластеризовать получившийся датасет векторов любым\n",
    "подходящим (по вашему мнению) алгоритмом. Попробуйте определить, по каким\n",
    "признакам различаются полученные кластера.\n",
    "Используя набор признаков, полученных в предыдущем пункте, примените\n",
    "XGBoost для решения задачи классификации. Сравните метрики качества XGBoost\n",
    "и нейронной сети.\n",
    "\n",
    "**Problem 2:** \n",
    "> Необходимо реализовать алгоритм knn(k nearest neighbors) на python и c++ (по\n",
    "возможности).\n",
    "Минимальный прототип:\n",
    "```\n",
    "def knn(points, k=10)\n",
    "```\n",
    "> При необходимости аргументы можно добавлять.\n",
    "points - матрица размера NxM, где N - количество точек, M-размерность\n",
    "пространства. В качестве выхода матрица Nxk с индексами ближайших k соседей,\n",
    "отсортированных по L2 расстоянию. Для теста использовать рандомно\n",
    "сгенерированные точки. Программа должна работать для значений N, M и k порядка\n",
    "10^4, 20, 30 соответственно.\n",
    "\n",
    "**Problem 2 extras:** \n",
    "> Добавить в функцию аргумент metric, с помощью которого можно выбирать способ\n",
    "вычисления расстояния. Реализовать подобный алгоритм для L1 и cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2dfaad",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type 'module' is not an instance of 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f1a771358e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvon_mises\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVonMises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mweibull\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeibull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/distributions/von_mises.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_rejection_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcentration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb)\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_rcb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0m_rcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jit_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateResolutionCallbackFromClosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_script_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualified_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_default_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m         \u001b[0;31m# Forward docstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'module' is not an instance of 'function'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os, os.path\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print('Using device:', device)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8e889",
   "metadata": {},
   "source": [
    "## 1.0. CNN Image classifier <a class=\"anchor\" id=\"image-classifier\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662873e9",
   "metadata": {},
   "source": [
    "Objective to design CNN image classifier is seems to be a common knowledge in a field of CV science. I will refer [the Stanford CS 231n class notes](https://cs231n.github.io/ \"CS231n: Convolutional Neural Networks for Visual Recognition\") and [tutorial in PyTorch documentation](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#transfer-learning-for-computer-vision-tutorial \"TRANSFER LEARNING FOR COMPUTER VISION TUTORIAL\") for my report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d977c834",
   "metadata": {},
   "source": [
    "### 1.1. - Data overview <a class=\"anchor\" id=\"image-classifier-data-overview\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db57f0ba",
   "metadata": {},
   "source": [
    "Before anything else, let's start from unzipping raw data. File `images_full.zip` should be placed in project root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bashCommand = \"unzip images_full.zip\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "bashCommand = \"ls images\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "classes = list(filter(None, output.decode(\"utf-8\").split('\\n')))\n",
    "nclasses = len(classes)\n",
    "print(nclasses, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7ec8eb",
   "metadata": {},
   "source": [
    "One could easily find that images in the given dataset belongs to one of the $8$ different classes. Let's define the available number of samples in each of the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4952734",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_names = {}\n",
    "per_class_counts = {}\n",
    "\n",
    "for cur_class in classes:\n",
    "    DIR = 'images/{}'.format(cur_class)\n",
    "    per_class_names[cur_class] = [name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]\n",
    "    per_class_counts[cur_class] = len(per_class_names[cur_class])\n",
    "\n",
    "overall_count = sum(per_class_counts.values())\n",
    "print(overall_count)\n",
    "per_class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a869528",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.bar(per_class_counts.keys(), per_class_counts.values())\n",
    "fig.suptitle('Samples count per class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8cbc32",
   "metadata": {},
   "source": [
    "So classes ArtDeco and cartoon; Impressionism, Japonisme and Naturism; photo and Rococo have about the same representation respectively. Class 'Cubism' is significantly more frequent than any others, while whole dataset is also class-imbalanced (but not dramatically hard).\n",
    "\n",
    "Since we a not going to either perform mining extra samples for retarded classes (which I hope would be unnecessary according the framework of this test task) or use data augmentation to level imbalance (because of intuitive risk of overfitting on relatevly low ammount of unique samples), we will only use information about class-imbalance in order to choose train-validation-test split strategy. We want relative class frequencies to be approximately preserved in each train and validation data subset and so stratified sampling is about to perform in our solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa1241",
   "metadata": {},
   "source": [
    "It is also might be useful to study distribution of raw numerical data amount between different classes. As far as I am concerned, such quantity is associated with cumulative image area (in square pixels, since all images have same number of color channels) for a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_area = {}\n",
    "for cur_class in classes:\n",
    "    tmp_sum = 0\n",
    "    for cur_img in per_class_names[cur_class]:\n",
    "        DIR = 'images/{}/{}'.format(cur_class, cur_img)\n",
    "        tmp_h, tmp_w, _ = cv2.imread(DIR).shape\n",
    "        tmp_sum += tmp_h*tmp_w\n",
    "    per_class_area[cur_class] = tmp_sum\n",
    "per_class_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.bar(per_class_area.keys(), per_class_area.values())\n",
    "fig.suptitle('Cumulative image area per class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037df0a0",
   "metadata": {},
   "source": [
    "The resulting distribution of classes by area completely mirrors the distribution obtained above by count of class instances. Thus, we can conclude that the average area of the images for all classes does not differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d88ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/photo/006512.jpg')[:,:,::-1]\n",
    "plt.title('Just a random grumpy cat img from dataset at the end of subsection')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335ec6a",
   "metadata": {},
   "source": [
    "### 1.2. - Data preprocessing <a class=\"anchor\" id=\"image-classifier-data-preprocessing\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aeff98",
   "metadata": {},
   "source": [
    "In order to implement stratified test-train split approach it is natural to move all preprocessed files into a single directory and preapair `.csv` with image names and target class labels. Thus now we are going to use opencv to resize all images to $128\\times 128$ pixels and save them in `dataset` directory. We will also utilize standard `OneHotEncoder` from` sklearn.preprocessing` to encode target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "bashCommand = \"rm -r dataset\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "os.mkdir('dataset')\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "enc.fit(np.array(classes).reshape(-1, 1))\n",
    "\n",
    "table_data = []\n",
    "table_data.append(['id'] + ['label{}'.format(x) for x in range(0, nclasses)])\n",
    "\n",
    "for cur_class in classes:\n",
    "    for cur_img_name in per_class_names[cur_class]:\n",
    "        \n",
    "        # We also replace '_' symbol by '-' over all *.jpg in order to avoid potential conflict with LaTex \n",
    "        encoded_label = enc.transform(np.array(cur_class).reshape(-1, 1)).tolist()[0]\n",
    "        table_data.append([cur_img_name.replace('_', '-')] + encoded_label)\n",
    "        DIR = 'images/{}/{}'.format(cur_class, cur_img_name)\n",
    "        OUTDIR = 'dataset/{}'.format(cur_img_name.replace('_', '-'))\n",
    "\n",
    "        raw_img = cv2.imread(DIR)#[:,:,::-1]\n",
    "        resized = cv2.resize(raw_img, (128, 128))\n",
    "        cv2.imwrite(OUTDIR, resized)\n",
    "\n",
    "column_names = table_data.pop(0)\n",
    "df = pd.DataFrame(table_data, columns=column_names)\n",
    "df.to_csv(\"dataset/labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e325790",
   "metadata": {},
   "source": [
    "On the next step we will use `StratifiedKFold` from` sklearn.model_selection` to generate stratified train-test split in $ k = 4 $ folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be114cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldataset = pd.read_csv(\"dataset/labels.csv\")\n",
    "\n",
    "k=4\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k, random_state=None, shuffle=False)\n",
    "decoded_target = enc.inverse_transform(fulldataset[['label{}'.format(x) for x in range(0, nclasses)]].to_numpy())\n",
    "\n",
    "for fold_id, (train_index, test_index) in enumerate(skf.split(fulldataset['id'].tolist(), \n",
    "                                                              decoded_target)):\n",
    "    tmp_df_train = fulldataset.iloc[train_index]\n",
    "    tmp_df_test = fulldataset.iloc[test_index]\n",
    "    tmp_df_train.to_csv(\"dataset/train_fold{}.csv\".format(fold_id), index=False)\n",
    "    tmp_df_test.to_csv(\"dataset/test_fold{}.csv\".format(fold_id), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45981973",
   "metadata": {},
   "source": [
    "Now we are about to prepare PyTorch wrapper for our data.\n",
    "\n",
    "ref: [Dataset class tutorial in PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files \"Creating a Custom Dataset for your files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a59a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtsImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_label_vectors = self.img_labels[['label{}'.format(tmp_i) for tmp_i in range(0, nclasses)]]\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.classes = ['ArtDeco', 'cartoon', 'Cubism', 'Impressionism', 'Japonism', 'Naturalism', 'photo', 'Rococo']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "#         image = cv2.imread(img_path)[:,:,::-1]\n",
    "        image = Image.open(img_path)\n",
    "        label = torch.tensor((self.img_label_vectors.iloc[idx]).to_list())\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets, dataloaders, dataset_sizes = [], [], []\n",
    "for fold_id in range(0, k):\n",
    "    image_datasets.append({x: ArtsImageDataset(\"dataset/{}_fold{}.csv\".format(x, fold_id), \n",
    "                                               'dataset', transform=data_transforms[x])\n",
    "                      for x in ['train', 'test']})\n",
    "    dataloaders.append({x: torch.utils.data.DataLoader(image_datasets[fold_id][x], batch_size=50,\n",
    "                                            shuffle=True, num_workers=4)\n",
    "                  for x in ['train', 'test']})\n",
    "\n",
    "    dataset_sizes.append({x: len(image_datasets[fold_id][x]) for x in ['train', 'test']})\n",
    "\n",
    "class_names = image_datasets[0]['train'].classes\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23dfcec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    fig, ax = plt.subplots(figsize=(13, 13))\n",
    "    ax.imshow(inp)\n",
    "    if title is not None:\n",
    "        print(title)\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "image_stream, labels_stream = next(iter(dataloaders[0]['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(image_stream)\n",
    "\n",
    "imshow(out, title=[enc.inverse_transform(x.numpy().reshape(1, -1))[0, 0] for x in labels_stream])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40501d0",
   "metadata": {},
   "source": [
    "### 1.3. - Transfer Learning approach <a class=\"anchor\" id=\"image-classifier-transfer-learning\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# redefine the last FC layer\n",
    "model_ft.fc = nn.Linear(num_ftrs, nclasses)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[0][phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                print(labels.data)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders[0]['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "        \n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223c255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
